{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29a8bfb-cd03-4de6-874c-2456cd0e54d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Chapter 1: Introduction**\n",
    "The first chapter introduces the field of Information Retrieval (IR) and its fundamental role in retrieving relevant information from unstructured or semi-structured data.\n",
    "\n",
    "#### **Key Concepts:**\n",
    "1. **What is Information Retrieval?**\n",
    "   - IR deals with retrieving documents from large collections, often in response to a user query.\n",
    "   - It focuses on **unstructured data**, such as free text, unlike traditional database systems that work with structured tables.\n",
    "\n",
    "2. **Applications of IR:**\n",
    "   - **Web search engines** (e.g., Google, Bing).\n",
    "   - **Digital libraries**, where research papers or books are indexed and searched.\n",
    "   - **Enterprise search systems** for internal organizational documents.\n",
    "\n",
    "3. **Boolean vs. Ranked Retrieval:**\n",
    "   - **Boolean Retrieval:** Retrieves documents if they match the query exactly using Boolean operators (AND, OR, NOT).\n",
    "   - **Ranked Retrieval:** Orders documents by their relevance to the query, often using **Cosine Similarity** or other ranking methods.\n",
    "\n",
    "4. **Challenges in IR:**\n",
    "   - **Scalability:** Handling billions of documents.\n",
    "   - **Relevance:** Determining which documents best meet the user’s needs.\n",
    "   - **Ambiguity:** Queries often have multiple interpretations (e.g., \"apple\" could refer to the fruit or the company).\n",
    "\n",
    "5. **Evaluation Metrics:**\n",
    "   - **Precision:** Proportion of retrieved documents that are relevant.\n",
    "   - **Recall:** Proportion of relevant documents retrieved.\n",
    "   - **F1 Score:** Harmonic mean of precision and recall to balance both metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chapter 2: The Term Vocabulary and Postings Lists**\n",
    "This chapter introduces the **inverted index**, which is the backbone of IR systems, and explains how raw text is processed to build this index.\n",
    "\n",
    "#### **Key Concepts:**\n",
    "1. **Text Preprocessing:**\n",
    "   - **Tokenization:** Splitting text into terms (e.g., words or phrases).\n",
    "   - **Stop-word Removal:** Eliminating common words like \"the,\" \"is,\" which do not help in retrieval.\n",
    "   - **Normalization:** Converting terms to a standard form (e.g., lowercase).\n",
    "   - **Stemming/Lemmatization:** Reducing terms to their root forms (e.g., \"running\" → \"run\").\n",
    "\n",
    "2. **Inverted Index:**\n",
    "   - An **inverted index** maps terms to the list of documents in which they appear, enabling efficient retrieval.\n",
    "   - Example:\n",
    "     ```\n",
    "     Term        Postings List\n",
    "     -------------------------\n",
    "     \"cat\"       [1, 3, 5]\n",
    "     \"dog\"       [2, 3, 6]\n",
    "     ```\n",
    "   - **Postings List:** Contains document IDs where the term appears, and optionally term frequencies or positions for advanced queries.\n",
    "\n",
    "3. **Boolean Retrieval:**\n",
    "   - Queries like \"cat AND dog\" are evaluated by intersecting the postings lists of \"cat\" and \"dog.\"\n",
    "   - This retrieval model is simple but does not rank documents by relevance.\n",
    "\n",
    "4. **Efficiency of the Inverted Index:**\n",
    "   - Postings lists are sorted by document IDs to allow fast intersection and query evaluation.\n",
    "   - Compression techniques (e.g., delta encoding) reduce the size of postings lists.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chapter 3: Dictionaries and Tolerant Retrieval**\n",
    "This chapter focuses on managing the **dictionary** (vocabulary of terms) and implementing **tolerant retrieval** methods for handling fuzzy queries, wild-cards, and spelling errors.\n",
    "\n",
    "#### **Key Concepts:**\n",
    "1. **Dictionary Data Structures:**\n",
    "   - The dictionary stores all unique terms in the collection.\n",
    "   - Efficient data structures include:\n",
    "     - **Hash Tables:** Provide fast lookups but do not support range queries.\n",
    "     - **Trees (e.g., B-Trees):** Support range queries and are better for large vocabularies.\n",
    "\n",
    "2. **Wild-card Queries:**\n",
    "   - Queries like \"comp*\" (to find terms like \"computer\" or \"computation\") require special handling.\n",
    "   - Techniques include:\n",
    "     - **Permuterm Index:** Index all rotations of a term (e.g., \"computer\" → \"computer$\", \"$computer,\" \"omputer$c\").\n",
    "     - **k-Grams:** Decompose terms into fixed-length substrings (e.g., \"computer\" → \"com,\" \"omp,\" \"mput\").\n",
    "\n",
    "3. **Spelling Correction:**\n",
    "   - **Edit Distance:** Measures the minimum number of insertions, deletions, or substitutions needed to transform one word into another.\n",
    "   - **Phonetic Matching:** Matches words based on their pronunciation (e.g., Soundex).\n",
    "\n",
    "4. **Fuzzy Matching:**\n",
    "   - Finds terms that are similar to the query term, often using edit distance or k-grams.\n",
    "\n",
    "---\n",
    "\n",
    "### **Chapter 4: Index Construction and Ranked Retrieval**\n",
    "This chapter explains how to build the **inverted index** at scale and introduces **ranked retrieval** using **Cosine Similarity**.\n",
    "\n",
    "#### **Key Concepts:**\n",
    "1. **Index Construction:**\n",
    "   - The **inverted index** is constructed through the following steps:\n",
    "     - **Parsing:** Tokenize documents into terms.\n",
    "     - **Normalization:** Apply preprocessing steps (e.g., stemming, stop-word removal).\n",
    "     - **Postings List Creation:** For each term, record the list of documents in which it appears.\n",
    "   - **Challenges:**\n",
    "     - **Dynamic Indexing:** Handling updates when documents are added, modified, or removed.\n",
    "     - **Distributed Indexing:** Large collections are divided across multiple machines for efficient index construction.\n",
    "\n",
    "2. **Cosine Similarity:**\n",
    "   - **Cosine Similarity** measures the angle between the vector representations of a query and a document in the **Vector Space Model**.\n",
    "   - Formula:\n",
    "     ```\n",
    "     cos(θ) = (D • Q) / (||D|| ||Q||)\n",
    "     ```\n",
    "     Where:\n",
    "     - `D • Q`: Dot product of document vector and query vector.\n",
    "     - `||D||`: Magnitude of the document vector.\n",
    "     - `||Q||`: Magnitude of the query vector.\n",
    "\n",
    "3. **Example of Cosine Similarity:**\n",
    "   - Query = `[1, 0, 1]`, Document = `[2, 1, 0]`\n",
    "   - Dot product: `(1*2) + (0*1) + (1*0) = 2`\n",
    "   - Magnitudes: `||D|| = sqrt(2^2 + 1^2 + 0^2) = sqrt(5)`, `||Q|| = sqrt(1^2 + 0^2 + 1^2) = sqrt(2)`\n",
    "   - Cosine similarity: `2 / (sqrt(5) * sqrt(2)) ≈ 0.63`\n",
    "\n",
    "4. **TF-IDF Weighting:**\n",
    "   - **Term Frequency (TF):** Measures how often a term appears in a document.\n",
    "   - **Inverse Document Frequency (IDF):** Gives higher importance to rare terms.\n",
    "   - TF-IDF Formula:\n",
    "     ```\n",
    "     TF-IDF = TF * log(N / DF)\n",
    "     ```\n",
    "     Where:\n",
    "     - `N`: Total number of documents.\n",
    "     - `DF`: Number of documents containing the term.\n",
    "\n",
    "5. **Benefits of Ranked Retrieval:**\n",
    "   - Unlike Boolean retrieval, ranked retrieval considers relevance and ranks documents accordingly.\n",
    "   - Cosine similarity combined with TF-IDF weighting ensures that important terms have a greater influence on ranking.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "The **first four chapters** of *\"An Introduction to Information Retrieval\"* provide the foundation for understanding how IR systems like search engines work. They explain:\n",
    "1. The **Boolean retrieval model** and its limitations.\n",
    "2. The construction and use of the **inverted index** for efficient query processing.\n",
    "3. Advanced techniques for **tolerant retrieval**, enabling flexibility in query handling.\n",
    "4. The introduction of **ranked retrieval** using **cosine similarity** and **TF-IDF weighting**, which marks the transition to more advanced and user-focused retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e6e33b-88ac-4221-8d9e-44a838d28da9",
   "metadata": {},
   "source": [
    "### **Cosine Similarity Explained**\n",
    "\n",
    "Cosine similarity is a metric used to measure how similar two documents (or vectors) are, based on the cosine of the angle between them in a multi-dimensional space. It is widely used in **information retrieval** to compare the similarity between a query and documents in the **Vector Space Model (VSM)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula for Cosine Similarity**\n",
    "\n",
    "If a query is represented as vector **Q** and a document is represented as vector **D**, the cosine similarity between them is calculated as:\n",
    "\n",
    "```\n",
    "cos(θ) = (D • Q) / (||D|| ||Q||)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **`D • Q`** = Dot product of the two vectors \\( D \\) (document) and \\( Q \\) (query).\n",
    "- **`||D||` and `||Q||`** = Magnitudes (or lengths) of the respective vectors.\n",
    "- **`cos(θ)`** = Cosine of the angle \\( \\theta \\) between the two vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Compute Cosine Similarity**\n",
    "\n",
    "1. **Represent the Text as Vectors:**\n",
    "   - Each document and query is represented as a vector in **n-dimensional space**, where \\( n \\) is the size of the vocabulary.\n",
    "   - The value for each dimension is typically the **TF-IDF weight** of the corresponding term.\n",
    "\n",
    "2. **Compute the Dot Product (D • Q):**\n",
    "   - Multiply the corresponding components of the two vectors and sum the results.\n",
    "\n",
    "3. **Compute the Magnitudes of Each Vector:**\n",
    "   - Calculate the magnitude of a vector using the formula:\n",
    "     ```\n",
    "     ||V|| = sqrt(v1^2 + v2^2 + ... + vn^2)\n",
    "     ```\n",
    "\n",
    "4. **Divide the Dot Product by the Product of Magnitudes:**\n",
    "   - Use the cosine similarity formula to get the similarity score.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Cosine Similarity Calculation**\n",
    "\n",
    "#### **Step 1: Define the Query and Document**\n",
    "Suppose we have the following **query** and two **documents**:\n",
    "\n",
    "- **Query (Q):** \"information retrieval\"\n",
    "- **Document 1 (D1):** \"information retrieval system\"\n",
    "- **Document 2 (D2):** \"data mining system\"\n",
    "\n",
    "#### **Step 2: Create Term Vectors**\n",
    "Here's the vocabulary extracted from all terms in the query and documents:\n",
    "\n",
    "| Term                | information | retrieval | system | data | mining |\n",
    "|---------------------|-------------|-----------|--------|------|--------|\n",
    "| Query (Q)           | 1           | 1         | 0      | 0    | 0      |\n",
    "| Document 1 (D1)     | 1           | 1         | 1      | 0    | 0      |\n",
    "| Document 2 (D2)     | 0           | 0         | 1      | 1    | 1      |\n",
    "\n",
    "#### **Step 3: Compute Cosine Similarity for Each Document**\n",
    "\n",
    "\n",
    "```\n",
    "cos(θ) = (D • Q) / (||D|| ||Q||)\n",
    "```\n",
    "**For Document 1 (D1):**\n",
    "1. **Dot Product (D1 • Q):**\n",
    "   ```\n",
    "   (1 * 1) + (1 * 1) + (0 * 1) + (0 * 0) + (0 * 0) = 1 + 1 = 2\n",
    "   ```\n",
    "\n",
    "2. **Magnitude of Query (||Q||):**\n",
    "   ```\n",
    "   ||Q|| = sqrt(1^2 + 1^2 + 0^2 + 0^2 + 0^2) = sqrt(1 + 1) = sqrt(2)\n",
    "   ```\n",
    "\n",
    "3. **Magnitude of Document 1 (||D1||):**\n",
    "   ```\n",
    "   ||D1|| = sqrt(1^2 + 1^2 + 1^2 + 0^2 + 0^2) = sqrt(1 + 1 + 1) = sqrt(3)\n",
    "   ```\n",
    "\n",
    "4. **Cosine Similarity for D1:**\n",
    "   ```\n",
    "   cos(θ) = (D1 • Q) / (||D1|| ||Q||)\n",
    "          = 2 / (sqrt(3) * sqrt(2))\n",
    "          ≈ 0.816\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "**For Document 2 (D2):**\n",
    "1. **Dot Product (D2 • Q):**\n",
    "   ```\n",
    "   (1 * 0) + (1 * 0) + (0 * 1) + (0 * 0) + (0 * 0) = 0\n",
    "   ```\n",
    "\n",
    "2. **Magnitude of Document 2 (||D2||):**\n",
    "   ```\n",
    "   ||D2|| = sqrt(0^2 + 0^2 + 1^2 + 1^2 + 1^2) = sqrt(1 + 1 + 1) = sqrt(3)\n",
    "   ```\n",
    "\n",
    "3. **Cosine Similarity for D2:**\n",
    "   ```\n",
    "   cos(θ) = (D2 • Q) / (||D2|| ||Q||)\n",
    "          = 0 / (sqrt(3) * sqrt(2))\n",
    "          = 0\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Results**\n",
    "- Cosine Similarity for Document 1: **0.816**\n",
    "- Cosine Similarity for Document 2: **0**\n",
    "\n",
    "### **Interpretation:**\n",
    "- Document 1 is much more similar to the query than Document 2, because it shares more terms with the query (\"information\" and \"retrieval\").\n",
    "- Document 2 has no terms in common with the query, resulting in a cosine similarity of 0.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Cosine Similarity?**\n",
    "1. **Scale-Invariance:** Cosine similarity only considers the direction of the vectors, not their magnitude, making it robust to differences in document length.\n",
    "2. **Effective Ranking:** It enables ranking of documents based on their relevance to the query, which is crucial for search engines.\n",
    "3. **Partial Matching:** It can handle cases where documents partially overlap with the query, unlike Boolean retrieval.\n",
    "\n",
    "Cosine similarity is a cornerstone of **ranked retrieval systems** and is often used in combination with **TF-IDF weighting** to emphasize important terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff346d-b2b5-4ff2-b0a8-c593d3cc2556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
